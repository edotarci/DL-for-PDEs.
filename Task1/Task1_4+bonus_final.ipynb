{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task4+Bonus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spectral (or Fourier) layer in 1d\n",
    "class SpectralConv1d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, modes1):\n",
    "        super(SpectralConv1d, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.modes1 = modes1\n",
    "\n",
    "        self.scale = (1 / (in_channels * out_channels))\n",
    "        self.weights1 = nn.Parameter(self.scale * torch.rand(in_channels, out_channels, self.modes1, dtype=torch.cfloat))\n",
    "\n",
    "    def compl_mul1d(self, input, weights):\n",
    "        return torch.einsum(\"bix,iox->box\", input, weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batchsize = x.shape[0]\n",
    "        x_ft = torch.fft.rfft(x)\n",
    "        out_ft = torch.zeros(batchsize, self.out_channels, x.size(-1) // 2 + 1, device=x.device, dtype=torch.cfloat)\n",
    "        out_ft[:, :, :self.modes1] = self.compl_mul1d(x_ft[:, :, :self.modes1], self.weights1)\n",
    "        x = torch.fft.irfft(out_ft, n=x.size(-1))\n",
    "        return x\n",
    "    \n",
    "# Time-conditional BN:\n",
    "class FILM(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                channels,\n",
    "                use_bn = True):\n",
    "        super(FILM, self).__init__()\n",
    "        self.channels = channels\n",
    "\n",
    "        self.inp2scale = nn.Linear(in_features=1, out_features=channels, bias=True)\n",
    "        self.inp2bias = nn.Linear(in_features=1, out_features=channels, bias=True)\n",
    "\n",
    "        self.inp2scale.weight.data.fill_(0)\n",
    "        self.inp2scale.bias.data.fill_(1)\n",
    "        self.inp2bias.weight.data.fill_(0)\n",
    "        self.inp2bias.bias.data.fill_(0)\n",
    "\n",
    "        if use_bn:\n",
    "          self.norm = nn.BatchNorm1d(channels)\n",
    "        else:\n",
    "          self.norm = nn.Identity()\n",
    "\n",
    "    def forward(self, x, time):\n",
    "\n",
    "        x = self.norm(x)\n",
    "        time = time.reshape(-1,1).type_as(x)\n",
    "        scale     = self.inp2scale(time)\n",
    "        bias      = self.inp2bias(time)\n",
    "        scale = scale.unsqueeze(2).expand_as(x)\n",
    "        bias  = bias.unsqueeze(2).expand_as(x)\n",
    "\n",
    "        return x * scale + bias\n",
    "\n",
    "\n",
    "class SpatialPositionalEmbedding(nn.Module):\n",
    "    def __init__(self, embedding_dim, max_length=10000):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def forward(self, x):\n",
    "        position = x.unsqueeze(-1)  # [batch_size, grid_size, 1, 1]\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, self.embedding_dim, 2, dtype=torch.float32) *\n",
    "            (-math.log(self.max_length) / self.embedding_dim)\n",
    "        )\n",
    "        embeddings = torch.zeros(*position.shape[:-1], self.embedding_dim, device=x.device)\n",
    "        embeddings[..., 0::2] = torch.sin(position * div_term)\n",
    "        embeddings[..., 1::2] = torch.cos(position * div_term)\n",
    "        return embeddings  # [batch_size, grid_size, embedding_dim]\n",
    "\n",
    "\n",
    "class LearnableTimeEmbedding(nn.Module):\n",
    "    def __init__(self, input_dim=1, embedding_dim=8, hidden_dim=32):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, embedding_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, t):\n",
    "        batch_size, grid_size, input_dim = t.shape\n",
    "        t_flat = t.reshape(-1, input_dim)  # [batch_size * grid_size, input_dim]\n",
    "        embeddings = self.mlp(t_flat)  # [batch_size * grid_size, embedding_dim]\n",
    "        embeddings = embeddings.reshape(batch_size, grid_size, -1)  # [batch_size, grid_size, embedding_dim]\n",
    "        return embeddings\n",
    "\n",
    "def h1_loss_finite_diff(u_pred, u_target, delta_x=1/64):\n",
    "    l2_loss = torch.mean((u_pred - u_target)**2)\n",
    "\n",
    "    # Compute first derivative using central differences\n",
    "    grad_u_pred = (u_pred[:, 2:] - u_pred[:, :-2]) / (2 * delta_x)\n",
    "    grad_u_target = (u_target[:, 2:] - u_target[:, :-2]) / (2 * delta_x)\n",
    "\n",
    "    # Pad gradients to match original shape\n",
    "    grad_u_pred = torch.nn.functional.pad(grad_u_pred, (1, 1), mode='replicate')\n",
    "    grad_u_target = torch.nn.functional.pad(grad_u_target, (1, 1), mode='replicate')\n",
    "\n",
    "    # Compute gradient L2 norm\n",
    "    grad_loss = torch.mean((grad_u_pred - grad_u_target)**2)\n",
    "\n",
    "    h1_loss = l2_loss + 0.1*grad_loss\n",
    "    return h1_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FNO1d(nn.Module):\n",
    "    def __init__(self, modes, width, use_bn=True):\n",
    "        super(FNO1d, self).__init__()\n",
    "\n",
    "        self.modes1 = modes\n",
    "        self.width = width\n",
    "        self.layers = 128\n",
    "        self.padding = 1  # pad the domain if input is non-periodic\n",
    "\n",
    "        self.embedding_dim = 2\n",
    "        self.spatial_embedding = SpatialPositionalEmbedding(self.embedding_dim)\n",
    "\n",
    "        self.time_embedding_dim = 2\n",
    "        self.time_embedding = LearnableTimeEmbedding(embedding_dim=self.time_embedding_dim)\n",
    "\n",
    "        self.linear_p = nn.Linear(1 + self.embedding_dim + self.time_embedding_dim, self.width)\n",
    "\n",
    "        # Spectral layers\n",
    "        self.spect1 = SpectralConv1d(self.width, self.width, self.modes1)\n",
    "        self.spect2 = SpectralConv1d(self.width, self.width, self.modes1)\n",
    "        self.spect3 = SpectralConv1d(self.width, self.width, self.modes1)\n",
    "        self.spect4 = SpectralConv1d(self.width, self.width, self.modes1)\n",
    "\n",
    "        # Conv1D layers\n",
    "        self.lin0 = nn.Conv1d(self.width, self.width, 1)\n",
    "        self.lin1 = nn.Conv1d(self.width, self.width, 1)\n",
    "        self.lin2 = nn.Conv1d(self.width, self.width, 1)\n",
    "        self.lin3 = nn.Conv1d(self.width, self.width, 1)\n",
    "\n",
    "        # Batch normalization layers\n",
    "        self.bn1 = FILM(self.width, use_bn)\n",
    "        self.bn2 = FILM(self.width, use_bn)\n",
    "        self.bn3 = FILM(self.width, use_bn)\n",
    "        self.bn4 = FILM(self.width, use_bn)\n",
    "\n",
    "        # Linear transformations\n",
    "        self.linear_q = nn.Linear(self.width, self.layers)\n",
    "        self.output_layer = nn.Linear(self.layers, 1)\n",
    "\n",
    "        self.activation = torch.nn.GELU()\n",
    "\n",
    "        # Learnable weights for skip connections\n",
    "        self.skip_weights = nn.Parameter(torch.ones(4)) \n",
    "\n",
    "    def fourier_layer(self, x, spectral_layer, conv_layer, batch_norm, time, skip_weight, skip=True):\n",
    "        x_old = x\n",
    "        x = spectral_layer(x) + conv_layer(x)\n",
    "        x = batch_norm(x, time)  \n",
    "        if skip:\n",
    "            x = self.activation(skip_weight * x_old + x) \n",
    "        else:\n",
    "            x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "    def linear_layer(self, x, linear_transformation):\n",
    "        return self.activation(linear_transformation(x))\n",
    "\n",
    "    def forward(self, x, time):\n",
    "        space = self.spatial_embedding(x[:, :, 1])\n",
    "        time_embed = self.time_embedding(x[:, :, 2].unsqueeze(-1))\n",
    "        x = torch.cat((x[:, :, 0].unsqueeze(-1), space, time_embed), dim=-1)\n",
    "        x = self.linear_p(x)\n",
    "        x = x.permute(0, 2, 1) \n",
    "\n",
    "        x = self.fourier_layer(x, self.spect1, self.lin0, self.bn1, time, self.skip_weights[0], skip=False)\n",
    "        x = self.fourier_layer(x, self.spect2, self.lin1, self.bn2, time, self.skip_weights[1])\n",
    "        #x = self.fourier_layer(x, self.spect3, self.lin2, self.bn3, time, self.skip_weights[2])\n",
    "        #x = self.fourier_layer(x, self.spect4, self.lin3, self.bn4, time, self.skip_weights[3])\n",
    "\n",
    "        x = x.permute(0, 2, 1)  # Back to (batch, grid, width)\n",
    "        x = self.linear_layer(x, self.linear_q)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ALL to ALL dataset\n",
    "def generate_all_to_all(dataset):\n",
    "    mesh_size = 64\n",
    "    input_data = []\n",
    "    target_data = []\n",
    "    time_data = []\n",
    "\n",
    "    # Iterate through each sequence in the dataset\n",
    "    for sequence in dataset:\n",
    "        num_states = 5 \n",
    "\n",
    "        # Iterate over all possible input states (u_t)\n",
    "        for t in range(num_states):\n",
    "            # Iterate over all possible future states (u_{t+k})\n",
    "            for k in range(t, num_states):\n",
    "                # Compute the time difference between t and k\n",
    "                time = (k - t) * 0.25\n",
    "                time_data.append([time])  #\n",
    "\n",
    "                # Retrieve the state at time t (u_t)\n",
    "                u_t = sequence[t, :]  \n",
    "\n",
    "                # Generate spatial coordinates for the mesh\n",
    "                x = torch.linspace(0, 1, mesh_size)\n",
    "                # Generate a time difference tensor\n",
    "                t_tensor = time * torch.ones(mesh_size) \n",
    "\n",
    "                # Combine u_t and x and t_tensor as the input pair\n",
    "                input_tensor = torch.stack([u_t, x, t_tensor], dim=0)  \n",
    "                input_data.append(input_tensor)  \n",
    "\n",
    "                # Target is the state at time k \n",
    "                target_data.append(sequence[k, :])  \n",
    "\n",
    "    time_data = torch.tensor(time_data)  \n",
    "    input_data = torch.stack(input_data).permute(0, 2, 1)  \n",
    "    target_data = torch.stack(target_data).unsqueeze(-1)  \n",
    "\n",
    "    # Return the generated training pairs\n",
    "    return time_data, input_data, target_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train input in dataloader torch.Size([960, 64, 3])\n",
      "train target in dataloader torch.Size([960, 64, 1])\n",
      "time train in dataloader torch.Size([960, 1])\n",
      "\n",
      "validation input in dataloader torch.Size([960, 64, 3])\n",
      "validation target in dataloader torch.Size([960, 64, 1])\n",
      "time validation in dataloader torch.Size([960, 1])\n"
     ]
    }
   ],
   "source": [
    "num_elements = 128\n",
    "training_sample = 64\n",
    "mesh_size = 64\n",
    "\n",
    "# Load the data\n",
    "path_train = \"FNO - Wave Equation/train_sol.npy\"\n",
    "data = torch.from_numpy(np.load(path_train)).type(torch.float32)\n",
    "\n",
    "#split into training and validation\n",
    "random_indices = np.random.choice(num_elements, training_sample, replace=False)\n",
    "training_dataset_task1 = data[random_indices, :, :]\n",
    "\n",
    "validation_indices = np.setdiff1d(np.arange(num_elements), random_indices)\n",
    "validation_dataset_task1 = data[validation_indices, :, :]\n",
    "\n",
    "time_train, input_train, target_train = generate_all_to_all(training_dataset_task1)\n",
    "time_validation, input_validation, target_validation = generate_all_to_all(validation_dataset_task1)\n",
    "\n",
    "print(\"train input in dataloader\", input_train.shape)\n",
    "print(\"train target in dataloader\", target_train.shape)\n",
    "print(\"time train in dataloader\", time_train.shape)\n",
    "print()\n",
    "print(\"validation input in dataloader\", input_validation.shape)\n",
    "print(\"validation target in dataloader\", target_validation.shape)\n",
    "print(\"time validation in dataloader\", time_validation.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameters\n",
    "batch_size = 32\n",
    "training_set = DataLoader(TensorDataset(time_train,input_train, target_train), batch_size=batch_size, shuffle=True)\n",
    "validation_set = DataLoader(TensorDataset(time_validation,input_validation, target_validation), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "learning_rate = 0.01\n",
    "epochs = 10000 \n",
    "\n",
    "modes = 16\n",
    "width = 128\n",
    "fno = FNO1d(modes, width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######### Epoch: 0 ######### Train Loss: 0.37727687 ######### Relative L2 Test Norm: 55.247107\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[98], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, (time_batch, input_batch, output_batch) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(training_set):\n\u001b[1;32m     19\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()  \n\u001b[0;32m---> 20\u001b[0m     output_pred_batch \u001b[38;5;241m=\u001b[39m \u001b[43mfno\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_batch\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m2\u001b[39m)  \n\u001b[1;32m     22\u001b[0m     loss_f \u001b[38;5;241m=\u001b[39m h1_loss_finite_diff(output_pred_batch, output_batch\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m2\u001b[39m))  \u001b[38;5;66;03m# H1 loss\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     loss_f\u001b[38;5;241m.\u001b[39mbackward() \n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[94], line 65\u001b[0m, in \u001b[0;36mFNO1d.forward\u001b[0;34m(self, x, time)\u001b[0m\n\u001b[1;32m     62\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear_p(x)\n\u001b[1;32m     63\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m) \n\u001b[0;32m---> 65\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfourier_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspect1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlin0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbn1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mskip_weights\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfourier_layer(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspect2, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlin1, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn2, time, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip_weights[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m#x = self.fourier_layer(x, self.spect3, self.lin2, self.bn3, time, self.skip_weights[2])\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m#x = self.fourier_layer(x, self.spect4, self.lin3, self.bn4, time, self.skip_weights[3])\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[94], line 48\u001b[0m, in \u001b[0;36mFNO1d.fourier_layer\u001b[0;34m(self, x, spectral_layer, conv_layer, batch_norm, time, skip_weight, skip)\u001b[0m\n\u001b[1;32m     46\u001b[0m x_old \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m     47\u001b[0m x \u001b[38;5;241m=\u001b[39m spectral_layer(x) \u001b[38;5;241m+\u001b[39m conv_layer(x)\n\u001b[0;32m---> 48\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m skip:\n\u001b[1;32m     50\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation(skip_weight \u001b[38;5;241m*\u001b[39m x_old \u001b[38;5;241m+\u001b[39m x) \n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1549\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1546\u001b[0m             tracing_state\u001b[38;5;241m.\u001b[39mpop_scope()\n\u001b[1;32m   1547\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[0;32m-> 1549\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wrapped_call_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1550\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1551\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initialize optimizer and scheduler\n",
    "optimizer = AdamW(fno.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=1e-3, max_lr=1e-2, step_size_up=2000, mode='triangular')\n",
    "\n",
    "\n",
    "# Frequency of printing\n",
    "freq_print = 10\n",
    "\n",
    "# Variables to track the best model\n",
    "best_val_loss = float('inf')\n",
    "best_model_state = None\n",
    "best_epoch = 0\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    train_mse = 0.0\n",
    "    fno.train() \n",
    "    for step, (time_batch, input_batch, output_batch) in enumerate(training_set):\n",
    "        optimizer.zero_grad()  \n",
    "        output_pred_batch = fno(input_batch, time_batch).squeeze(2)  \n",
    "\n",
    "        loss_f = h1_loss_finite_diff(output_pred_batch, output_batch.squeeze(2))  # H1 loss\n",
    "\n",
    "        loss_f.backward() \n",
    "        optimizer.step() \n",
    "        train_mse += loss_f.item() \n",
    "\n",
    "    train_mse /= len(training_set)\n",
    "    scheduler.step() \n",
    "\n",
    "    # Validation loop\n",
    "    with torch.no_grad():\n",
    "        fno.eval()  \n",
    "        test_relative_l2 = 0.0\n",
    "        for step, (time_batch, input_batch, output_batch) in enumerate(validation_set):\n",
    "            output_pred_batch = fno(input_batch, time_batch).squeeze(2)  \n",
    "            # Compute relative L2 norm\n",
    "            loss_f = (torch.mean(torch.norm(output_pred_batch - output_batch.squeeze(2), p=2)) /\n",
    "                      torch.norm(output_batch.squeeze(2), p=2)) * 100\n",
    "            test_relative_l2 += loss_f.item()  \n",
    "        test_relative_l2 /= len(validation_set)  \n",
    "\n",
    "    # Track the best model\n",
    "    if test_relative_l2 < best_val_loss:\n",
    "        best_val_loss = test_relative_l2\n",
    "        best_model_state = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': fno.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'best_val_loss': best_val_loss\n",
    "        }\n",
    "        best_epoch = epoch\n",
    "\n",
    "    # Print progress every freq_print epochs\n",
    "    if epoch % freq_print == 0:\n",
    "        print(f\"######### Epoch: {epoch} ######### Train Loss: {train_mse:.8f} ######### Relative L2 Test Norm: {test_relative_l2:.6f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(best_model_state, \"best_fno_model_t4.pth\")\n",
    "#print(f\"Best model saved from epoch {best_epoch} with validation loss {best_val_loss:.4f} as 'best_fno_model_t4.pth'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Carica lo stato salvato\n",
    "checkpoint = torch.load(\"best_fno_model_t4.pth\", weights_only=True)\n",
    "\n",
    "# Ripristina lo stato del modello\n",
    "fno.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = \"FNO - Wave Equation/test_sol.npy\"\n",
    "data_test = torch.from_numpy(np.load(test_path)).type(torch.float32)\n",
    "num_test = len(data_test)\n",
    "# Generate test data\n",
    "time_test, input_test, target_test = generate_all_to_all(data_test)\n",
    "test_set = DataLoader(TensorDataset(time_test, input_test, target_test), batch_size=1, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ALL2ALL testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples over time [640. 512. 384. 256. 128.]\n",
      "Mean relative L2 loss at t =  0.0 : 4.252252138219774\n",
      "Mean relative L2 loss at t =  0.25 : 39.09376021102071\n",
      "Mean relative L2 loss at t =  0.5 : 36.505533856650196\n",
      "Mean relative L2 loss at t =  0.75 : 27.7706618309021\n",
      "Mean relative L2 loss at t =  1.0 : 20.241275161504745\n"
     ]
    }
   ],
   "source": [
    "# Initialize evaluation\n",
    "with torch.no_grad():\n",
    "    fno.eval()  \n",
    "\n",
    "    num_time_steps = 5\n",
    "    test_relative_l2_vet = np.zeros(num_time_steps)\n",
    "    counter_time = np.zeros(num_time_steps)\n",
    "\n",
    "\n",
    "    # Iterate through test dataset\n",
    "    for step, (time, input, output) in enumerate(test_set):\n",
    "        output_pred = fno(input, time).squeeze(2)  \n",
    "        #loss_f = (torch.mean(torch.norm(output_pred - output.squeeze(2), p=2)) / torch.norm(output.squeeze(2),p=2)) * 100\n",
    "        loss_f =torch.norm(output_pred - output.squeeze(2), p=2) / torch.norm(output.squeeze(2),p=2) * 100\n",
    "\n",
    "        # Update counters\n",
    "        time_index = int(time[0, 0] * 4)\n",
    "        counter_time[time_index] += 1\n",
    "        test_relative_l2_vet[time_index] += loss_f.item()\n",
    "\n",
    "    # Compute mean relative L2 for each time step\n",
    "    mean_relative_l2 = test_relative_l2_vet / counter_time\n",
    "    print(\"samples over time\", counter_time)\n",
    "    for i in range(5):\n",
    "        print(\"Mean relative L2 loss at t = \", 0.25*i,\":\", mean_relative_l2[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OOD testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test OOD dataset shape: torch.Size([128, 2, 64])\n"
     ]
    }
   ],
   "source": [
    "test_path = \"FNO - Wave Equation/test_sol_OOD.npy\"\n",
    "data_test_OOD = torch.from_numpy(np.load(test_path)).type(torch.float32)\n",
    "num_test_OOD = len(data_test_OOD)\n",
    "\n",
    "print(\"Test OOD dataset shape:\", data_test_OOD.shape)\n",
    "\n",
    "# Split initial and final condition\n",
    "input_test_OOD = data_test_OOD[:, 0, :].unsqueeze(-1)\n",
    "\n",
    "target_test_OOD = data_test_OOD[:, -1, :]\n",
    "\n",
    "# Add spatial mesh to the tensor\n",
    "x_OOD = torch.linspace(0, 1, mesh_size).repeat(num_test, 1).unsqueeze(-1)  \n",
    "t_OOD = torch.ones(mesh_size).unsqueeze(0).repeat(num_test_OOD, 1).unsqueeze(-1)\n",
    "input_test_withtime_OOD = torch.cat((input_test_OOD, x_OOD,t_OOD), dim=-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative L2 error (OOD): 46.40522766113281\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "    output_function_test_pred_OOD = fno(input_test_withtime_OOD, torch.ones((num_test_OOD, 1))).squeeze(2)\n",
    "\n",
    "err_OOD = (torch.mean(torch.norm(output_function_test_pred_OOD - target_test_OOD, p=2)) / torch.norm(target_test_OOD,p=2)) * 100\n",
    "print(\"Relative L2 error (OOD):\", err_OOD.item())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
